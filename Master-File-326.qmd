---
title: "Master-File-326"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Load Libraries

```{r}
library(dplyr)
library(ggplot2)
library(descr) 
library(tidyverse)
library(labelled)
library(janitor)
library(pastecs)
library(haven)
library(readstata13)
library(sna)
library(GGally)
library(reshape2)
library(viridis)
library(lme4)
library(sjPlot)
library(MASS)
library(lme4)
library(naniar)
library(lattice)


library(beepr)
library(dendextend)
library(tidytext)
library(factoextra)
library(ggfortify)
library(tidymodels)
library(tidyverse)
```

# Import Data

```{r}
w1_m <- read_dta("Data-Stata/w1_main.dta")
w1_c <- read_dta("Data-Stata/w1_classmate.dta")
w1_a <- read_dta("Data-stata/w1_achievement.dta")
```

# Exploring Data we Use for Network Measures

#### Count number of classes by school:

```{r}
w1_c |>  
  group_by(schoolidCUF) %>% 
  summarise(n_class = length(unique(classidCUF))) %>% 
  group_by(n_class) %>% 
  summarise(count_by_num_class=n(),
            percent = count_by_num_class/80)
```

#is it a problem we only have 1 or 2 classes in each school?

#### Count Students by School:

```{r}
w1_c %>% 
  group_by(schoolidCUF) %>% 
  summarise(total_obs = n()) %>% # NUM of participants from each school
  arrange(desc(total_obs)) |> 
  ungroup()
```

#### Count Students by Class:

```{r}
w1_c %>% 
  group_by(schoolidCUF, classidCUF) |> 
  summarise(n_student = length(unique(youthidCUF))) |> 
  group_by(n_student) |> 
  summarise(n_classes = n()) |> 
  mutate(prop_classes = n_classes/sum(n_classes))

  
  # create a table of the number of students in each class
class_table <- table(w1_c$classidCUF, w1_c$youthidCUF)

# count the number of students in each class
n_students <- apply(class_table, 1, function(x) sum(x > 0))

# create a table of the frequency of each number of students
freq_table <- table(n_students)

# calculate the proportion of classes with each number of students
prop_table <- prop.table(freq_table)

# print the proportion table
prop_table

```

# Create Network Measures for all data in Wave I

------------------------------------------------------------------------

## \*\*Some Classes Should not be used -- LOOK AT THE FLAGS

## Count Reciprocity (n mutual friends, non-reciprocal)

```{r preprocess data}
w1_reciprocal <- w1_c |> 
  dplyr::select(youthidCUF, 7:210) |> 
  mutate(y1_bfs_0 = as.numeric(as.character(y1_bfs_0)))

# code out missing values
w1_reciprocal <- w1_reciprocal |> 
  #labelled::to_factor() %>%  # changed labelled/haven_labelled object to factor
  lapply(function(x) as.numeric(as.character(x))) |>  # coerce all vars to be numeric
  data.frame() 

w1_reciprocal[w1_reciprocal < 0] <- NA

var_classes <- sapply(w1_reciprocal, class)

# show the result
var_classes

# exclude the 4 students did not respond to best friends questions at all
descr::freq(w1_c$y1_bfs_0, plot=FALSE)
descr::freq(w1_reciprocal$y1_bfs_0, plot=FALSE)
```

```{r reshape data to be long format}

w1_reciprocal_long <- w1_reciprocal |> 
  filter(!is.na(y1_bfs_0)) %>% 
  dplyr::select(youthidCUF, y1_bfs_1:y1_bfs_5) %>% 
  pivot_longer(cols = y1_bfs_1:y1_bfs_5,
               names_to = "best_friends",
               values_to = "friendID") %>% 
  dplyr::select(youthidCUF, friendID) 

length(unique(w1_reciprocal_long$youthidCUF))
```

After getting rid of 6 students who did not respond to bf questions at all

### Count \# of Friends Listed

```{r count by best friends}
w1_bfs <- w1_reciprocal_long |> 
  mutate(hasfriend = ifelse(is.na(friendID),0,1)) |> 
  group_by(youthidCUF) |> 
  summarise(num_bfs = sum(hasfriend)) 

descr::freq(w1_bfs$num_bfs, plot = FALSE)
```

### Count \# of Mutual Friends

```{r count mutual friends}

students <- unique(w1_bfs$youthidCUF)
fred_mat <- as.data.frame(students) %>% 
  mutate(n_mutual_bfs = 0) |> 
  mutate(non_reciprocal = 0)
colnames(fred_mat)[1] <- "youthidCUF"

for (i in 1 : nrow(fred_mat)) {
  id = fred_mat[i,1]
  dt = w1_reciprocal_long %>% filter(youthidCUF == id)
  bfs = unique(dt$friendID)
  for (bf in bfs) {
    bf_data = w1_reciprocal_long %>% filter(youthidCUF == bf)
    if (id %in% unique(bf_data$friendID)){
      fred_mat[i,2] = fred_mat[i,2] + 1
      } 
    else if (! id %in% unique(bf_data$friendID)){
      fred_mat[i,3] = fred_mat[i,3] + 1
    }
    else {next}
  }
}

descr::freq(fred_mat$n_mutual_bfs)
descr::freq(fred_mat$non_reciprocal)
```

```{r merge friend counts}

w1_friend_counts <- merge(fred_mat,w1_bfs,by="youthidCUF")

n_distinct(w1_friend_counts$youthidCUF)
```

### Calculate Positive Friendship Network measures

```{r}
# w1_pos <- w1_c |> 
#   dplyr::select(youthidCUF, 7:210) |> 
#   mutate(y1_bfs_0 = as.numeric(as.character(y1_bfs_0)))
# 
# # code out missing values
# w1_pos <- w1_pos |> 
#   #labelled::to_factor() %>%  # changed labelled/haven_labelled object to factor
#   lapply(function(x) as.numeric(as.character(x))) |>  # coerce all vars to be numeric
#   data.frame() 
# 
# w1_pos[w1_pos < 0] <- NA
# 
# 
# w1_pos_long <- w1_pos %>% 
#   filter(!is.na(y1_bfs_0)) %>% 
#   dplyr::select(youthidCUF, y1_bfs_1:y1_bfs_5) %>% 
#   pivot_longer(cols = y1_bfs_1:y1_bfs_5,
#                names_to = "best_friends",
#                values_to = "friendid") %>% 
#   dplyr::select(youthidCUF, friendid) 
# 
# length(unique(w1_pos_long$youthidCUF))
# 
# 
# 
# 
# 
# 
# 
# ##same as w1_reciprocal_long
# students <- unique(w1_reciprocal_long$youthidCUF)
# mat <- matrix(0, nrow=length(students), ncol = length(students))
# colnames(mat) <- as.character(students)
# rownames(mat) <- as.character(students)
# # A glimpse of what this look lik
# mat[1:4,1:4]
# 
# # fill adjacency matrix ALL:
# i <- 1
# for (i in 1:length(students)) {
#   dt <- as.data.frame(w1_reciprocal_long |>  filter(!is.na(friendID)))
#   row.index <- which(students == dt[i,1])
#   col.index <- which(students == dt[i,2]) # col 3 is best friends 1
#   mat[row.index, col.index] <- 1
# }
# mat[1:5,1:5]
# 
# # r build network ALL
# bfs_nw_all <- network(mat, 
#                       matrix.type = "adjacency",
#                       ignore.eval=FALSE,
#                       names.eval="true")
# 
# 
# 
# el.list <- list()
# class.list <- list()
# 
# for (i in 1:nclass){
#   el.list[[i]] <- class1.l[class1.l$classidCUF == classids[i],]
#   class.list[[i]] <- classids[i]
# }
# 
# net.list <- lapply(el.list, as.network.matrix, matrix.type = "edgelist", 
#  ignore.eval = FALSE, names.eval = "classidCUF")	
# 
# #calculate centrality:
# # mat.list <- lapply(net.list, as.matrix)
# # indegree.list <- lapply(mat.list, degree, cmode = "indegree", gmode = "digraph", diag = FALSE)
# # outdegree.list <- lapply(mat.list, degree, cmode = "outdegree", gmode = "digraph", diag = FALSE)
# 
# degree(bfs_nw_all, cmode = "indegree")
# degree(bfs_nw_all, cmode = "outdegree")
# degree(bfs_nw_all)
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# mat.list <- lapply(net.list, as.matrix)
# indegree.list <- lapply(mat.list, degree, cmode = "indegree", gmode = "digraph", diag = FALSE)
# outdegree.list <- lapply(mat.list, degree, cmode = "outdegree", gmode = "digraph", diag = FALSE)
```

dplyr::select variables we use to create positive network (5 bf nomination in class):

```{r}

class1 <- w1_c |> 
  dplyr::select(-y1_intdat_ycRV, -y1_vers_yc)

pos_vars <- c("y1_bfs_0", "y1_bfs_1", "y1_bfs_2", "y1_bfs_3", "y1_bfs_4", "y1_bfs_5")
class1[pos_vars][class1[pos_vars] < 0] <- NA
descr::freq(class1$y1_bfs_0)

```

Count \# Students

```{r}

class1.l <- class1 |> 
  mutate(y1_bfs_0 = as.numeric(as.character(y1_bfs_0))) |> 
  filter(!is.na(y1_bfs_0)) |> 
  dplyr::select(youthidCUF, y1_bfs_1:y1_bfs_5) |> 
  pivot_longer(cols = y1_bfs_1:y1_bfs_5,
               names_to = "best_friends",
               values_to = "alteridCUF") |> 
  dplyr::select(youthidCUF, alteridCUF) |> 
  na.omit() 
# remove NAs for building network ?

print("Number of Unique Students in subsample")
length(unique(class1.l$youthidCUF))


```

Count \# Classes

```{r}

myids <- as.data.frame(unique(class1[c("youthidCUF","classidCUF")]))
classids <- unique(class1$classidCUF)
class1.l <- merge(class1.l, myids, by = "youthidCUF", all.x = TRUE)
classids <- unique(class1.l$classid)

nclass <- length(unique(class1.l$classid))
print("Number of classes in subsample")
nclass

```

Create positive networks in each class;

```{r}

# make sure you take only the last two digits of youthid otherwise sna
# thinks that there are a thousand actors
class1.l <- class1.l %>% 
  mutate(youthid = youthidCUF %% 100,
         alterid = alteridCUF %% 100) %>% 
  dplyr::select(youthid, alterid, classidCUF)


el.list <- list()
class.list <- list()

for (i in 1:nclass){
  el.list[[i]] <- class1.l[class1.l$classidCUF == classids[i],]
  class.list[[i]] <- classids[i]
}

net.list <- lapply(el.list, as.network.matrix, matrix.type = "edgelist", 
 ignore.eval = FALSE, names.eval = "classidCUF")	


```

Calculate degree centrality

```{r}
task_net <- list()
for (i in length(net.list)){
  task_net <- network(as.matrix(net.list[[1]]),        
                    matrix.type = "adjacency",
                    ignore.eval = FALSE, 
                    names.eval = "value")
}



mat.list <- lapply(net.list, as.matrix)
indegree.list <- lapply(mat.list, degree, cmode = "indegree", gmode = "digraph", diag = FALSE)
outdegree.list <- lapply(mat.list, degree, cmode = "outdegree", gmode = "digraph", diag = FALSE)
```

```{r unlist positive network measures}
a.list <- lapply(net.list, get.vertex.attribute, "vertex.names")
netm <- mapply(cbind, a.list, class.list, indegree.list, outdegree.list)
netm <- lapply(netm, as.data.frame)
netm <- as.data.frame(do.call(rbind, netm))
names(netm) <- c("youthid", "classid", "pos_indegree", "pos_outdegree")

# computate isolate measure
pos_netm <- netm %>% 
  mutate(pos_isolate = ifelse(pos_indegree == 0 & pos_outdegree == 0, 1, 0),
         youthid_chr = as.character(youthid),
         classid_chr = as.character(classid),
         youthid_nchr = nchar(youthid_chr) # count the number of characters
         )

# add 0 to single digit youthid
pos_netm$youthid_chr[netm$youthid_nchr == 1] <- paste0("0", netm$youthid_chr[netm$youthid_nchr == 1])

# change to numeric youthid
pos_netm$youthid <- as.numeric(paste0(pos_netm$classid_chr, pos_netm$youthid_chr))
pos_netm <- pos_netm |> 
  select(youthid, classid, pos_indegree, pos_outdegree, pos_isolate)


```

dplyr::select variables we use to create negative network (5 do not want to sit by nomination in class):

```{r}
neg_vars <- c("y1_sit_0", "y1_sit_1", "y1_sit_2", "y1_sit_3", "y1_sit_4", "y1_sit_5")
class1[neg_vars][class1[neg_vars] < 0] <- NA
descr::freq(class1$y1_sit_0)
```

Check: count \# students in neg network data

```{r}
# reshape to long
neg_class1.l <- class1 %>% 
  mutate(y1_sit_0 = as.numeric(as.character(y1_bfs_0))) |> 
  filter(!is.na(y1_sit_0)) |> 
  dplyr::select(youthidCUF, y1_sit_1:y1_sit_5) %>% 
  pivot_longer(cols = y1_sit_1:y1_sit_5,
               names_to = "bad_friends",
               values_to = "alteridCUF") %>% 
  dplyr::select(youthidCUF, alteridCUF) %>% 
  #mutate(alteridCUF = ifelse(is.na(alteridCUF), 0, alteridCUF))
  na.omit() # remove NAs for building network
#THIS IS NOT RIGHT - WON'T THIS REMOVE ALL PEOPLE WHO DIDN'T LIST ALL 5?
print("Number of students in negative subsample")
length(unique(neg_class1.l$youthidCUF))
```

Check: count \# classes

```{r}
neg_class1.l <- merge(neg_class1.l, myids, by = "youthidCUF", all.x = TRUE)
classids_neg <- unique(neg_class1.l$classidCUF)

nclass_neg <- length(unique(neg_class1.l$classidCUF))
print("Number of Classes in Negative network")
nclass_neg

```

Create negative networks for each class; calculate neg in and out degree, isolates

```{r}
# make sure you take only the last two digits of youthid otherwise sna
# thinks that there are a thousand actors
neg_class1.l <- neg_class1.l %>% 
  mutate(youthid = youthidCUF %% 100,
         alterid = alteridCUF %% 100) %>% 
  dplyr::select(youthid, alterid, classidCUF)

neg_el.list <- list()
neg_class.list <- list()

for (i in 1:nclass_neg){
  neg_el.list[[i]] <- neg_class1.l[neg_class1.l$classidCUF == classids_neg[i],]
  neg_class.list[[i]] <- classids_neg[i]
}

neg_net.list <- lapply(neg_el.list, as.network.matrix, matrix.type = "edgelist", 
 ignore.eval = FALSE, names.eval = "classidCUF")	

neg_mat.list <- lapply(neg_net.list, as.matrix)
neg_indegree.list <- lapply(neg_mat.list, degree, cmode = "indegree", gmode = "digraph", diag = FALSE)
neg_outdegree.list <- lapply(neg_mat.list, degree, cmode = "outdegree", gmode = "digraph", diag = FALSE)

neg_a.list <- lapply(neg_net.list, get.vertex.attribute, "vertex.names")
neg_netm <- mapply(cbind, neg_a.list, neg_class.list, neg_indegree.list, neg_outdegree.list)
neg_netm <- lapply(neg_netm, as.data.frame)
neg_netm <- as.data.frame(do.call(rbind, neg_netm))
names(neg_netm) <- c("youthid", "classid", "neg_indegree", "neg_outdegree")

# computate isolate measure
neg_netm <- neg_netm %>% 
  mutate(neg_isolate = ifelse(neg_indegree == 0 & neg_outdegree == 0, 1, 0),
         youthid_chr = as.character(youthid),
         classid_chr = as.character(classid),
         youthid_nchr = nchar(youthid_chr) # count the number of characters
         )

# add 0 to single digit youthid
neg_netm$youthid_chr[neg_netm$youthid_nchr == 1] <- paste0("0", neg_netm$youthid_chr[neg_netm$youthid_nchr == 1])

# change to numeric youthid
neg_netm$youthid <- as.numeric(paste0(neg_netm$classid_chr, neg_netm$youthid_chr))

neg_netm <- neg_netm |> 
  dplyr::select(youthid, classid, neg_indegree, neg_outdegree, neg_isolate) |> 
  rename(youthidCUF = youthid, classidCUF = classid)


```

Merge positive and negative network measures

```{r}
w1_netm <- merge(neg_netm, pos_netm, by = c("youthidCUF", "classidCUF"))

#why are these different lengths?? we do not have all network measure for all observations....
#THIS IS WRONG!!! GO LOOK AT WHY NEG_NETM AND POS_NETM HAVE SO MANY OBS
length(unique(neg_netm$youthidCUF))
length(unique(pos_netm$youthidCUF))
length(unique(w1_netm$youthidCUF))
length(unique(w1_friend_counts$youthidCUF))

```

# Save and Read Network Data

```{r}
#now merge centrality measures w reciprocity measures
w1_netm <- merge(w1_netm, w1_friend_counts, by = "youthidCUF")

write_csv(w1_netm, "Data-Stata/our_data/network_chars_326.csv") 
```

```{r}
w1_netm <- read_csv("Data-Stata/our_data/network_chars_326.csv")
```

Get covariates of interest from main questionnaire:

```{r}
covars <- c("youthidCUF","classidCUF","schoolidCUF","schtype_geRV","y1_sex","y1_doby","y1_nationRV","y1_lpsc1","y1_lpsc2","y1_lpsc3","y1_lpsc4","y1_fact","y1_sat2","y1_sat1", "y1_pbsch1","y1_pbsch2","y1_vict1","y1_vict2","y1_vict3","y1_pdisc1","y1_date", "y1_fcoh1","y1_fcoh2","y1_fcoh3","y1_fcoh4","y1_fcoh5","y1_pdem1","y1_pdem2","y1_pdem3","y1_pdem4","y1_pdem5","y1_pdem6","y1_pdem7","y1_pinv1","y1_pinv2","y1_pinv3", "y1_migage", "y1_educm3", "y1_educf3", "y1_gradem_geCUF", "y1_gradesc_geCUF", "y1_gradee_geCUF", "y1_repeat", "y1_iseimG", "y1_iseifG", "y1_edasp1CS", "y1_edasp2CS", "y1_edasp3CS", "y1_sspm", "y1_sspsc", "y1_sspe", "y1_seff1", "y1_seff2", "y1_valed1", "y1_asch2", "y1_asch1", "y1_tenc1", "y1_tenc2", "y1_tenc3", "y1_idsc", "y1_club")

#covars <- c("youthidCUF","classidCUF","schoolidCUF","schtype_geRV","y1_sex","y1_doby","y1_nationRV","y1_lpsc1","y1_lpsc2","y1_lpsc3","y1_lpsc4","y1_fact","y1_sat2","y1_date")

#add class size to use as a class-level predictor
main1 <- w1_m[covars]
class_size <- main1 |> 
  group_by(classidCUF) |> 
  summarize(class_size = n()) |> 
  ungroup()

main1 <- main1 |> 
  left_join(class_size, by = "classidCUF")


main1 <- merge(main1, w1_netm, by = "youthidCUF")


```

# Merge all covariates, network measures into one df, save data

```{r}
scores <- w1_a |> 
  dplyr::dplyr::select(classidCUF, youthidCUF, y1_cot_sum, y1_lat_sum)


main <- merge(main1, scores)
write_csv(main, "Data-Stata/our_data/main_326.csv")
```

# Read Complete Data (network measures, all covars)

```{r}
main <- read_csv("Data-Stata/our_data/main_326.csv")

```

Check normality of outcome variable:

```{r}
main <- main |> 
  filter(y1_sat2>0)


shapiro.test(main$y1_sat2) # p-value < 0.05 implying that the distribution of the outcome are significantly different from normal

# create a Q-Q plot
qqnorm(main$y1_sat2)
qqline(main$y1_sat2)
hist(main$y1_sat2)



```

# Covariates:

schooltype

nationality

age

y1_cot_sum: cognitive ability test score

y1_lat_sum: language test score

sex: boy or girl

Y1_migage: age at migration

Y1_edasp1cs: what is the highest level of ed you wish to get

Y1_edasp2cs: highest level of ed you think you will actually get

Y1_edasp3cs: highest level of ed your parents want you to get

Y1_sspm: subjective how well doing in math

Y1_sspsc: subjective how well doing in language

Y1_sspe: subjective how ell doing in englilsh

Y1_gradem_geCUF: grades you got in last school report (math)

Y1_gradesc_geCUF: grades got last school report (german)

Y1_gradee_geCUF: grades got last school report: English

Y1_repeat: ever repeated school year

Y1_seff1: agree or disagree: I am sure that I can do well at school

Y1_asch1: agree or disagree: school is not for people like me

Y1_seff1: agree or disagree: I am sure I can get good grades

Y1_asch2: agree or disagree: it is very important to me to get good grades

Y1_valed1: agree or disagree: education is very important for getting

class_size

Explore Covariate Distributions

```{r}

main <- main |> 
  mutate_at(vars(setdiff(covars, 'y1_migage')), ~ ifelse(. < 0, NA, .))


ggplot(main, aes(x = schtype_geRV)) + 
  geom_bar()

# plot nationality
ggplot(main, aes(x = y1_nationRV)) + 
  geom_bar()

# plot age
ggplot(main, aes(x = y1_doby)) + 
  geom_histogram()

# plot y1_cot_sum
ggplot(main, aes(x = y1_cot_sum)) + 
  geom_histogram()

# plot y1_lat_sum
ggplot(main, aes(x = y1_lat_sum)) + 
  geom_histogram()

# plot sex
ggplot(main, aes(x = y1_sex)) + 
  geom_bar()

# plot Y1_migage
ggplot(main, aes(x = y1_migage)) + 
  geom_histogram()

# plot Y1_nationRV
ggplot(main, aes(x = y1_nationRV)) + 
  geom_bar()

# plot Y1_edasp1cs
ggplot(main, aes(x = y1_edasp1CS)) + 
  geom_histogram()

# plot Y1_edasp2cs
ggplot(main, aes(x = y1_edasp2CS)) + 
  geom_histogram()

# plot Y1_edasp3cs
ggplot(main, aes(x = y1_edasp3CS)) + 
  geom_histogram()

# plot Y1_sspm
ggplot(main, aes(x = y1_sspm)) + 
  geom_histogram()

# plot Y1_sspsc
ggplot(main, aes(x = y1_sspsc)) + 
  geom_histogram()

# plot Y1_sspe
ggplot(main, aes(x = y1_sspe)) + 
  geom_histogram()

# plot Y1_gradem_geCUF
ggplot(main, aes(x = y1_gradem_geCUF)) + 
  geom_bar()

# plot Y1_grades_geCUF
ggplot(main, aes(x = y1_gradesc_geCUF)) + 
  geom_bar()

# plot Y1_gradee_geCUF
ggplot(main, aes(x = y1_gradee_geCUF)) + 
  geom_bar()

# plot Y1_repeat
ggplot(main, aes(x = y1_repeat)) + 
  geom_bar()

# plot Y1_seff1
ggplot(main, aes(x = y1_seff1)) + 
  geom_histogram()

# plot Y1_asch1
ggplot(main, aes(x = y1_asch1)) + 
  geom_histogram()

# plot Y1_asach2
ggplot(main, aes(x = y1_asch2)) + 
  geom_histogram()

# plot Y1_valed1
ggplot(main, aes(x = y1_valed1)) + 
  geom_histogram()
```

# MLM Analyses

## Clean/prep data for MLM analyses

```{r}
main <- main |> 
  #create age variable
  mutate(age = 2010 - main$y1_doby) |> 
  #create immigrant variable
  mutate(immigrant = if_else(y1_migage < 0, 0, 1)) |> 
  #reverse code some of the likert (numeric) items:
  mutate(
    Y1_fact = 5 - y1_fact,
    y1_vict1 = 5 - y1_vict1,
    y1_vict2 = 5 - y1_vict2,
    y1_vict3 = 5 - y1_vict3,
    y1_pdisc1 = 5 - y1_pdisc1,
    y1_fcoh1 = 5 - y1_fcoh1,
    y1_idsc = 5 - y1_idsc
  ) |> 
  mutate(
    Y1_pbsch1 = 6 - y1_pbsch1,
    y1_pbsch2 = 6 - y1_pbsch2,
    y1_pdem1 = 6 - y1_pdem1,
    y1_pdem2 = 6 - y1_pdem2,
    y1_pdem3 = 6 - y1_pdem3,
    y1_pdem4 = 6 - y1_pdem4,
    y1_pdem5 = 6 - y1_pdem5,
    y1_pdem6 = 6 - y1_pdem6,
    y1_pdem7 = 6 - y1_pdem7,
    y1_sspm = 6 - y1_sspm,
    y1_sspsc = 6 - y1_sspsc,
    y1_sspe = 6 - y1_sspe,
    y1_seff1 = 6 - y1_seff1,
    y1_seff2 = 6 - y1_seff2,
    y1_asch2 = 6 - y1_asch2,
    y1_valed1 = 6 - y1_valed1,
    y1_tenc1 = 6 - y1_tenc1,
    y1_tenc2 = 6 - y1_tenc2,
    y1_tenc3 = 6 - y1_tenc3
  )

  

d_clean <- main |> 
  # change gender/ nationality / school type as factor
  mutate(sex = as.factor(y1_sex),
         immigrant = as.factor(immigrant),
         schooltype = as.factor(schtype_geRV),
         nationality = as.factor(y1_nationRV),
         repeated_year = as.factor (y1_repeat),
         mom_university = as.factor(y1_educm3),
         dad_university = as.factor(y1_educf3),
         club_member = as.factor(y1_club)) |> 
        
  
         
         # 
         # 
         # believe_do_well = as.factor(y1_seff1),
         # school_for_me = as.factor(y1_asch1),
         # grades_important = as.factor(y1_asch2),
         # education_value = as.factor(y1_valed1),
         # math_grades_CUF = as.factor(y1_gradem_geCUF),
         # german_grades_CUF = as.factor(y1_gradesc_geCUF),
         # english_grades_CUF = as.factor (y1_gradee_geCUF),
         # highest_ed_wish = as.factor(y1_edasp1CS),
         # highest_ed_think = as.factor(y1_edasp2CS),
         # highes_ed_parents_wish = as.factor(y1_edasp3CS)) |> 

          mutate(sch_problem_behaviors = rowMeans(cbind(y1_pbsch1, y1_pbsch2)),
           victimization = rowMeans(cbind(y1_vict1, y1_vict2, y1_vict3)),
           family_cohesion = rowMeans(cbind(y1_fcoh1, y1_fcoh2, y1_fcoh3)),
           parent_support = rowMeans(cbind(y1_pdem1, y1_pdem5, y1_pdem6, y1_pdem7)),
           parent_strict = rowMeans(cbind(y1_pdem2, y1_pdem3, y1_pdem4)),
           subj_school_perform = rowMeans(cbind(y1_sspm, y1_sspsc, y1_sspe)),
           sch_confidence = rowMeans(cbind(y1_seff2, y1_seff1, y1_asch1)),
           sch_important = rowMeans(cbind(y1_asch2, y1_valed1)),
           teacher_support = rowMeans(cbind(y1_tenc1, y1_tenc2)),
           unfair_teach = y1_tenc3,
           identity_german = y1_idsc,
           ISEI_mom = y1_iseimG,
           ISEI_dad = y1_iseifG,
           mig_age = y1_migage,
           sch_discrimination = y1_pdisc1,
           cant_afford = y1_fact,
           german_skills = rowMeans(cbind(y1_lpsc1, y1_lpsc2, y1_lpsc3, y1_lpsc4)),
           age = as.numeric(age),
           class_size = as.numeric(class_size),
           sch_satisfaction = as.numeric(y1_sat2),
           lang_test = as.numeric(y1_lat_sum),
           cog_test = as.numeric(y1_cot_sum)) 

d_analysis <- d_clean |> 
  dplyr::dplyr::select(lang_test, cog_test, sch_satisfaction, class_size, age, german_skills, cant_afford, sch_discrimination, mig_age, ISEI_dad, ISEI_mom, identity_german, unfair_teach, teacher_support, sch_important, sch_confidence, subj_school_perform, parent_strict, parent_support, family_cohesion, victimization, club_member, dad_university, mom_university, repeated_year, nationality, schooltype, immigrant, sex, youthidCUF, classidCUF, schoolidCUF, pos_indegree, pos_outdegree, pos_isolate, neg_indegree, neg_outdegree, neg_isolate, n_mutual_bfs, non_reciprocal)

```

Mean-center and recode dummy variables

```{r}
numeric_vars <- c("age", "mig_age", "class_size", "subj_school_perform", "cog_test", "lang_test")

numeric_vars <- setNames(numeric_vars, str_c(numeric_vars, "_c")) 
numeric_vars

d_analysis <- d_analysis |> 
  mutate(across(numeric_vars, ~ as.numeric(.))) |> 
  mutate(across(numeric_vars, ~. - mean(., na.rm = TRUE))) |> 
  mutate(german = if_else(nationality == 3, 0, 1)) |> 
  mutate(repeated_year = if_else(repeated_year ==1, 0, 1)) |> 
  mutate(mom_university = if_else(mom_university ==2, 0, 1)) |> 
  mutate(dad_university = if_else(dad_university ==2, 0, 1)) |> 
  mutate(club_member = if_else(club_member ==2, 0, 1)) |> 
  mutate(sch_combo = if_else(schooltype == 2, 1, 0)) |> 
  mutate(sch_int = if_else(schooltype==3, 1, 0)) |> 
  mutate(sch_comp = if_else(schooltype==4, 1,0)) |> 
  mutate(sch_secondary = if_else(schooltype==5, 1,0)) |> 
  mutate(sch_specneed = if_else(schooltype==6, 1,0)) |> 
  mutate(sexgirl = if_else(sex == 2, 1, 0))
  
```

```{r}
miss_var_summary(d_analysis)
```

```{r}
write_csv(d_analysis, "Data-Stata/our_data/d_analysis_326.csv")
```

```{r}
d_analysis <- read_csv("Data-Stata/our_data/d_analysis_326.csv") |> 
  na.omit()
```

m1: no random intercepts

```{r}
# m1 <- glm(sch_satisfaction ~ 1, data = d_analysis,
#           family = gaussian(link = "log"))

m1 <- lm(sch_satisfaction ~ 1, data = d_analysis)
summary(m1)
```

\*Hi Emma, great work. And for your question, it would be better to add variables one by one and compare the models to be able to interpret the results more clearly.

## Hypothesis 1: Null Model (Intercept-Only)

The null hypothesis here is that there is no correlation between students within different classes and schools on the outcome: school satisfaction. The alternative would be that there is systematic between-class and between-school variation in students' school satisfaction ratings.

m2: random intercepts (class)

```{r}
# m2 <- glmer(sch_satisfaction ~ (1 | classidCUF), data = d_analysis,
#           family = gaussian(link = "log"))

m2 <- lmer(sch_satisfaction ~ (1 | classidCUF), data = d_analysis,
          REML = F)
summary(m2)
```

ICC:

```{r}
randompar <- as_tibble(VarCorr(m2))
randompar
var_class <- randompar[1,4]
var_pupil <- randompar[2,4]
var_class / (var_pupil + var_class)

.1475 / (.1475 + 3.8199)
```

3.7% of the variation in school satisfaction is on the class level

```{r}
(2* logLik(m2)) - (2* logLik(m1))

tab_model(m1, m2)
```

\*\*We are NOT seeing sig differences with the random intercept... but we did see graphical evidence...do we leave out?

m3: random intercepts (school)

```{r}
m3 <- lmer(sch_satisfaction ~ (1 | schoolidCUF/classidCUF), data = d_analysis,
          REML = F, na.action = na.omit)
summary(m3)


```

The variance explained at the school level overlaps strongly with that explained by class (we see classidCUF random effect variance --\>)

Likely not suitable for 3 levels; we choose to only use the class-level from here on out. In addition, our only school-level variable (school type) did not seem to explain any of the variance in our school satisfaction outcome

## Hypothesis 2: Add Level 1 Predictors

The null hypothesis is that there is no relationship between level 1 (student-level) predictors (student's social network measures (centrality, reciprocity), student-level demographic covariates) and outcome. The alternative is there is a relationship between network measures and school satisfaction

m4: add student-level covariates

```{r}
# m4 <- lmer(sch_satisfaction ~ (1 | schoolidCUF) + (1 | classidCUF)
#            + pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
#              + sex + lang_test + cog_test + age + nationality + immigrant, data = d_analysis,
#           REML = F, na.action = na.omit)
# summary(m4)

m4 <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + (1 | classidCUF), data = d_analysis,
          REML = F, na.action = na.omit)
summary(m4)


```

```{r}
anova(m4, m2)

tab_model(m2, m4)
```

Model 4 does significantly better than model 2 (p \< .001)! We reject the hypothesis that level 1 predictors do not explain our school satisfaction outcome

Calculate the r2s (explained variance) as explained in the lecture:

```{r}
#classidCUF:
(0.1475 - 0.1723)/0.1475

#residual:
(3.8199 - 3.5013)/3.8199 

```

\*\* WHY CLASS R2 NEGATIVE?

Now let's see if the effects of network measures on school satisfaction varies across other level 1 predictors (e.g., immigrant status, gender).

age:

```{r}
m4_ageint <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + (1 | classidCUF) + class_size + age*pos_indegree + age*pos_outdegree + age*neg_indegree + age*neg_outdegree + age*pos_isolate + age*neg_isolate + age*n_mutual_bfs + age*non_reciprocal, data = d_analysis,
          REML = F, na.action = na.omit)
summary(m4_ageint)

anova(m4_ageint, m4)
tab_model(m4, m4_ageint)
```

immigrant status:

```{r}
m4_immint <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + (1 | classidCUF) + class_size + immigrant*pos_indegree + immigrant*pos_outdegree +immigrant*neg_indegree + immigrant*neg_outdegree + immigrant*pos_isolate + immigrant*neg_isolate + immigrant*n_mutual_bfs + immigrant*non_reciprocal, data = d_analysis,
          REML = F, na.action = na.omit)
summary(m4_immint)

anova(m4_immint, m4)
tab_model(m4, m4_immint)
```

victimization:

```{r}
m4_victint <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + (1 | classidCUF) + class_size + victimization*pos_indegree + victimization*pos_outdegree +victimization*neg_indegree + victimization*neg_outdegree + victimization*pos_isolate + victimization*neg_isolate + victimization*n_mutual_bfs + victimization*non_reciprocal, data = d_analysis,
          REML = F, na.action = na.omit)
summary(m4_victint)

anova(m4_victint, m4)
tab_model(m4, m4_victint)
```

sex:

```{r}
m4_sexint <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + (1 | classidCUF) + class_size + sexgirl*pos_indegree + sexgirl*pos_outdegree +sexgirl*neg_indegree + sexgirl*neg_outdegree + sexgirl*pos_isolate + sexgirl*neg_isolate + sexgirl*n_mutual_bfs + sexgirl*non_reciprocal, data = d_analysis,
          REML = F, na.action = na.omit)
summary(m4_sexint)

anova(m4_sexint, m4)
tab_model(m4, m4_sexint)
```

the effect of neg outdegree and neg isolate on school satisfaction is different for girls and boys. Specifically, it suggests that the negative effect of neg outdegree on school satisfaction is stronger for girls than for boy

None of these interaction effects significantly improve our model... we will just stick with m4

## **Hypothesis 3 - Add level 2 (and 3?) predictors:**

The null hypothesis is that there is no significant relationship between level 2 and level 3 predictors and the outcome. In other words, classroom-level variables (e.g., class size) and school-level variables (school type, school stratum (by proportion immigrant students)) have no relationship to students' school satisfaction. The alternative is that these class- and school-level variables do significantly correlate to school satisfaction.

\*Note: we decided not to use school as level 3 in our model, but we can still include school type as a covariate

m5: add class-level predictors (class size)

Possible limitation: not more class-level or teacher-level variables (teacher experience, etc)

```{r}
# m5 <- lmer(sch_satisfaction ~ (1 | schoolidCUF) + (1 | classidCUF)
#            + pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
#              + sex + lang_test + cog_test + age + nationality + immigrant
#            + class_size, data = d_analysis,
#           REML = F, na.action = na.omit)


m5 <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + (1 | classidCUF) + class_size, data = d_analysis,
          REML = F, na.action = na.omit)
summary(m5)
```

```{r}
anova(m5, m4)

tab_model(m4, m5)
```

Adding our class-level variable (class size) doesn't seem to improve our model significantly. However, we want to interact it with the network measures later to see if networks matter more/less in smaller/bigger classes, so we'll keep it

m6: add school-level predictors (school type, reference = lower secondary school

```{r}
m6 <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + (1 | classidCUF) + class_size + sch_combo + sch_int + sch_comp + sch_secondary + sch_specneed, data = d_analysis,
          REML = F, na.action = na.omit)
summary(m6)
```

```{r}
anova(m6, m5)
tab_model(m5, m6)
```

\*\*Adding school type doesn't significantly improve our model. Should we get rid??

Let's stick with m5 for now

## **Hypothesis 4 - Add random slopes:**

The null hypothesis is the relationship between our student-level predictors (network measures and covariates) and the outcome are the same across all groups. In other words, the effect of the network measures on a student's school satisfaction is constant within all of the classes and all of the schools. The alternative is that the effect of these student-level variables is not the same within all classes (level 2) and schools (level 3).

m7: **A model with random slopes,** Let's allow the effects of our network measures to vary across classes

```{r}

# m7 <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
#              + sexgirl + lang_test + cog_test + age + immigrant + victimization + class_size + (1 + pos_indegree | classidCUF) + (1 + pos_outdegree | classidCUF) + (1 + n_mutual_bfs | classidCUF) + (1 + non_reciprocal | classidCUF)+ (1 + pos_isolate | classidCUF) +  (1 + neg_isolate | classidCUF)+ (1 + neg_indegree | classidCUF) + (1 + neg_outdegree | classidCUF) , data = d_analysis,
#           REML = F, na.action = na.omit)
# summary(m7)

#Let 'Positive Out degree' vary:
m7_rposin <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + class_size
           + (1 + pos_outdegree | classidCUF), data = d_analysis,
          REML = F, na.action = na.omit)
summary(m7_rposin)

#Let number of mutual friends vary
m7_rnbfs <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + class_size
           + (1 + n_mutual_bfs | classidCUF), data = d_analysis,
          REML = F, na.action = na.omit)
summary(m7_rnbfs)

#Let negative in degree vary:
m7_rnegin <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + class_size
           + (1 + neg_indegree | classidCUF), data = d_analysis,
          REML = F, na.action = na.omit)
summary(m7_rnegin)

tab_model(m5, m7_rposin, m7_rnbfs, m7_rnegin)
anova(m7_rposin, m5)
anova(m7_rnbfs, m5)
anova(m7_rnegin, m5)

```

We get a singularity warning when including any network measures but the 3 listed above... none of them on their own significantly improve our model from m5

No evidence for random slopes???

m8: try a few more variables allowing for random slope

```{r}

m8 <-  lmer(sch_satisfaction ~  + pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant
           + (1 + immigrant | classidCUF) + (1 + sex | classidCUF) + (1 + age | classidCUF), data = d_analysis,
          REML = F, na.action = na.omit)
summary(m8)

anova(m8, m5)
tab_model(m5, m8)
```

Still no evidence for model improvement with random slopes

## **Hypothesis 5 - Add cross-level interactions:**

The null hypothesis is that there is no systematic variation between student-level predictors (network measures, demographic covars) and outcome school satisfaction across different values of level 2 and level 3 (class- and school-level covariates). For example, the effect of network centrality doesn't vary with the value of the class size, or the school type. The alternative is that there is cross-level interaction, and a class-level or school-level variable partly explains why the relationship between student-level network measures and school satisfaction varies across classes and schools.

m9: We now always add a covariance between intercept and slope.

```{r}
# 
# m9 <- lmer(sch_satisfaction ~  (1 | classidCUF)
#            + pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal + 
#              + sexgirl + lang_test + cog_test + age + victimization + immigrant, data = d_analysis,
#           REML = F, na.action = na.omit)
# summary(m10)
# 
# 
# class_size*pos_indegree + class_size*pos_outdegree + class_size*neg_indegree + class_size*neg_outdegree + class_size*pos_isolate + class_size*neg_isolate + class_size*n_mutual_bfs + class_size*non_reciprocal

m9 <- lmer(sch_satisfaction ~  pos_indegree + pos_outdegree + neg_indegree + neg_outdegree + pos_isolate + neg_isolate + n_mutual_bfs + non_reciprocal
             + sexgirl + lang_test + cog_test + age + immigrant + victimization
           + (1 | classidCUF) + class_size + class_size*pos_indegree + class_size*pos_outdegree + class_size*neg_indegree + class_size*neg_outdegree + class_size*pos_isolate + class_size*neg_isolate + class_size*n_mutual_bfs + class_size*non_reciprocal, data = d_analysis,
          REML = F, na.action = na.omit)
summary(m9)
```

```{r}
anova(m9, m5)
tab_model(m5, m9)
```

Dealing with non-normality:

```{r}

new model <- glmmPQL()

## Using mixed model with penalized quasilikelihood (PQL) since our data are not normally distributed


library(MASS)
PQL <- glmmPQL(y1_sat2 ~ pos_indegree + pos_outdegree + pos_isolate + neg_indegree + neg_outdegree + neg_isolate + n_mutual_bfs  + non_reciprocal  + as.factor(gender) + language + nationality + schooltype , random = ~1 | schoolidCUF/classidCUF, family = gaussian(link = "log"),
    data = main2, verbose = FALSE)
summary(PQL)

m1 <- glmer(rep1 ~ male 
            + pped 
            + msesc 
            + (1 | classlid),  
            data = uthai1, 
            family = gaussian(link = "logit"), 
            na.action = na.omit)

# consider using a gamma or inverse Gaussian family with a log link function in the generalized linear model (GLM). 
# 

# The log link function is commonly used with gamma and inverse Gaussian distributions, as it can help to model the relationship between the predictor variables and the outcome variable, particularly when the outcome variable is positively skewed.
# Exampls:
model <- glm(y ~ x1 + x2, family = inverse.gaussian(link = "log"), data = mydata)
summary(model)
```

Immigrant status?

Victim in School?

Look at interaction effects b/n social network measures and migrant status

Do friends need to be same ethnicity? or cross-ethnic?

explore moderation: interplay of negative and positive ties; protective of victimization? of poor family environment?

# Cluster Analysis

```{r}
d_analysis <- read_csv("Data-Stata/our_data/d_analysis_326.csv") |> 
  na.omit()
```

What variables do I need to include to do cluster analysis??

### K-means clustering on network measures, class size

```{r}
library(stats)
d_cluster <- d_analysis |> 
  dplyr::dplyr::select(neg_indegree, neg_outdegree, pos_indegree, pos_outdegree, pos_isolate, neg_isolate, non_reciprocal, n_mutual_bfs) |> 
  rename(reciprocal = n_mutual_bfs)


d_cluster_z <- d_cluster |> 
    mutate(across(-c(pos_isolate, neg_isolate), scale))



```

run k-means model for 1-15; collect within-cluster sum of squares and plot scree plot:

```{r}

library(factoextra)
library(ggfortify)
library(tidymodels)
library(dendextend)


set.seed(1234)

tibble(k = 1:15) |>
  mutate(kmeansmod = map(k, ~ kmeans(d_cluster_z, 
                             .x,
                             nstart = 25,
                             iter.max = 1000)),
         glanced = map(kmeansmod, glance)) |>
  unnest(cols = c(glanced)) |>
  ggplot(aes(k, tot.withinss)) +
  geom_line() +
  geom_point()

```

No clear elbow; let's just use k = 5 for now

#### Create Clusters:

```{r}

set.seed(1234)
network_clusters <- kmeans(d_cluster_z, 
                        centers = 6, 
                        nstart = 25)

network_clusters

tidy(network_clusters)

network_clusters_4 <- kmeans(d_cluster_z,
                             centers = 4,
                              nstart = 25)

network_clusters$cluster
# k = 3
# my_clusters <- kmeans(d_cluster_z, k)
```

#### Plot Results

```{r}
tidy(network_clusters) |>
  mutate(cluster = str_c("cluster_", 1:6)) |>
  pivot_longer(cols = -c(cluster, withinss, size),
               names_to = "Variable",
               values_to = "Value") |>
  mutate(importance = Value * Value) |>
  group_by(cluster) |>
  slice_max(importance, n = 10) |>
  ungroup() |>
  mutate(cluster = as.factor(cluster),
         Variable = reorder_within(Variable,
                                   by = importance,
                                   within = cluster)) |>
  ggplot() +
  geom_col(aes(y = Value, 
               x = Variable,
               fill = Variable)) +
  facet_wrap(~ cluster, 
             nrow = 2,
             scales = "free_y") + 
  guides(fill = "none") +
  scale_x_reordered() +
  coord_flip()


```

plot clusters on principal components. autoplot will return the clusters plotted on the first two principal components

```{r}
autoplot(network_clusters, 
         d_cluster_z, 
         alpha = 0.5)  +
  scale_color_discrete(breaks=c("1", "2", "3", "4", "5"))
```

Looks like it's not doing a great job...

The kproto() function allows us to do k-prototypes algorithm (allows you to update the cluster centroids for continuous variables using the means and updating the cluster modes for binary variables using the mode.) We have both continuous and factor vars, so we use this method

First, determine how many k to use

```{r}
#install.packages("clustMixType")

# Load the 'clustMixType' package
library(clustMixType)

# Prepare your data with categorical variables as factors and numerical variables as numeric
# For example:
categorical_vars <- c("neg_indegree", "neg_outdegree", "pos_indegree", "pos_outdegree", "reciprocal", "non_reciprocal")  # Categorical variable names
numerical_vars <- c("pos_isolate", "neg_isolate")  # Numerical variable names

# Combine categorical and numerical variables into a single data frame
d_proto <- data.frame(d_cluster_z[, categorical_vars], d_cluster_z[, numerical_vars]) %>%
  mutate(pos_isolate = as.factor(pos_isolate), neg_isolate = as.factor(neg_isolate))


k_values <- 1:15

# Create an empty vector to store the total within-cluster sum of squares
tot_withinss <- numeric(length(k_values))

# Perform k-prototypes clustering for each value of k and calculate the total within-cluster sum of squares
for (k in k_values) {
  kproto_result <- kproto(d_proto, k, nstart = 25)
  tot_withinss[k] <- kproto_result$tot.withinss
}

# Plot the total within-cluster sum of squares
plot(k_values, tot_withinss, type = "b", pch = 19, frame = FALSE, xlab = "Number of Clusters (k)", ylab = "Total Within-Cluster Sum of Squares", main = "Total Within-Cluster Sum of Squares vs. Number of Clusters")
```

Not very clear... let's go for 6?

```{r}

# Specify the number of clusters (k)
k <- 6

# Perform k-prototypes clustering
kproto_result <- kproto(d_proto, k, nstart = 25)

# Get the cluster assignments
cluster_labels <- kproto_result$cluster

# Get the cluster centroids
cluster_centroids <- kproto_result$centers

# View the cluster assignments
print(cluster_labels)

# View the cluster centroids
print(cluster_centroids)

```

```{r}

cluster_prototypes <- data.frame(
  withinss = kproto_result$withinss,
  size = kproto_result$size,
  kproto_result$centers
) |> 
  rename(cluster = size.clusters, size = size.Freq) |> 
  mutate(pos_isolate = if_else(pos_isolate == '1', 1, 0), neg_isolate = if_else(neg_isolate == '1', 1, 0))

cluster_prototypes |>
  pivot_longer(cols = -c(cluster, withinss, size),
               names_to = "Variable",
               values_to = "Value") |>
  mutate(importance = Value * Value) |>
  group_by(cluster) |>
  slice_max(importance, n = 10) |>
  ungroup() |>
  mutate(cluster = as.factor(cluster),
         Variable = reorder_within(Variable,
                                   by = importance,
                                   within = cluster)) |>
  ggplot() +
  geom_col(aes(y = Value, 
               x = Variable,
               fill = Variable)) +
  facet_wrap(~ cluster, 
             nrow = 2,
             scales = "free_y") + 
  guides(fill = "none") +
  scale_x_reordered() +
  coord_flip()

```

Note: therea re no clusters where mode of isolate measures == 1; this makes sense since there are very few isolates in the df:

```{r}
histogram(d_analysis$pos_isolate)
histogram(d_analysis$neg_isolate)
```

#### Modeling

```{r}
d_analysis <- mutate(d_analysis, cluster = as.factor(cluster_labels)) 

# d_analysis <- mutate(d_analysis, cluster_labels = case_when(
#   cluster_labels == 1 ~ "average",
#   cluster_labels == 2 ~ "popular",
#   cluster_labels == 3 ~ "unreciprocated",
#   cluster_labels == 4 ~ "loner",
#   cluster_labels == 5 ~ "unproblematic",
#   cluster_labels == 6 ~ "disliked"
# ))

         
#d_analysis_4c <- d_analysis |> mutate(cluster = as.factor(network_clusters_4$cluster))

#ADD CLASS SIZES/INTERACTIONS!!!!, school type 
m1 <- lmer(sch_satisfaction ~  cluster + sexgirl + lang_test + cog_test + age + immigrant + ISEI_parent_c + (1 | classidCUF) + class_size, data = d_analysis,
          REML = F, na.action = na.omit)
summary(m1)

#sch_combo + sch_int + sch_comp + sch_secondary + sch_specneed


m2 <- glmer(sch_satisfaction ~  cluster_labels + sexgirl + lang_test + cog_test + age + immigrant + ISEI_parent_c + (1 | classidCUF) + class_size + sch_combo + sch_int + sch_comp + sch_secondary + sch_specneed
,  
            data = d_analysis, 
            family = gaussian(link = "log"), 
            na.action = na.omit)
summary(m2)


m3 <- glmer(sch_satisfaction ~  cluster_labels + sexgirl + lang_test + cog_test + age + immigrant + ISEI_parent_c + (1 | classidCUF), family = inverse.gaussian(link = "log"), data = d_analysis)
summary(m3)

#this might be most suitable (for left-skewed outcome var:)
m4 <- glmer(sch_satisfaction ~  cluster + sexgirl + lang_test + cog_test + age + immigrant + ISEI_parent_c + (1 | classidCUF) + class_size + sch_combo + sch_int + sch_comp + sch_secondary + sch_specneed
, family = Gamma(link = "log"), data = d_analysis)
summary(m4)



```

Dealing with non-normality: new model

```{r}

## Using mixed model with penalized quasilikelihood (PQL) since our data are not normally distributed

library(MASS)
PQL <- glmmPQL(sch_satisfaction ~  cluster + sexgirl + lang_test + cog_test + age + immigrant + ISEI_parent_c + class_size, random = ~1 | schoolidCUF/classidCUF, family = Gamma(link = "log"),
    data = d_analysis, verbose = FALSE)
summary(PQL)


```

### Hierarchical Clustering

```{r}
cluster_diff <- dist(d_cluster_z, method = "euclidean")
```

hierarchical clustering using Complete Linkages, Average or Single linkages.

```{r}
network_hc_c <- hclust(cluster_diff, method = "complete" )
network_hc_a <- hclust(cluster_diff, method = "average" )
network_hc_s <- hclust(cluster_diff, method = "single" )
```

Now we can plot a dendogram using base R plot. Note: There are other packages available for plotting dendograms. The cex argument reduces the font, the hang argument will plot the labels at the same position, and main, xlab and sub refer to titles and axis labels.

```{r}
plot(network_hc_c, 
     cex = 0.6, 
     hang = -1, 
     main = "complete", 
     xlab = "",
     sub = "")

plot(network_hc_s, 
     cex = 0.6, 
     hang = -1, 
     main = "single", 
     xlab = "",
     sub = "")

plot(network_hc_a, 
     cex = 0.6, 
     hang = -1, 
     main = "average", 
     xlab = "",
     sub = "")
```

Let's now further explore what the clusters are. The hierarchical cluster object stores the height of the branches in the dendogram and the height reflects differences: the larger the height, the more difference there is by the right and left branch at that split.

```{r}
fviz_nbclust(d_cluster_z, FUN = hcut, method = "wss")
```

We can then do the same by coloring the leaves for the dendogram. First we will force the hierarchical cluster object into a dendogram object and addcolors to the branches and leaves according to k = 4.

```{r}
dend <- as.dendrogram(network_hc_c) |>
  color_labels(k = 4) |>
  color_branches(k = 4)

dend |>
  set("labels_cex", 0.5) |>
  plot(main = "Complete")
```

```{r}
dend <- as.dendrogram(network_hc_c) |>
  color_labels(k = 5) |>
  color_branches(k = 5)

dend |>
  set("labels_cex", 0.5) |>
  plot(main = "Complete")
```

# Principal Components Analysis

```{r}
d_pca <- d_cluster_z |> 
  dplyr::dplyr::select(neg_indegree, neg_outdegree, pos_indegree, pos_outdegree, n_mutual_bfs, non_reciprocal)


pca <- prcomp(d_pca, center = TRUE, scale. = TRUE)

pca
plot(pca)
summary(pca)


#to plot variances:
pov <- pca$sdev^2/sum(pca$sdev^2)
#We now have it in the variable pov, proportion of variance. You can plot it

barplot(pov)
```

## What's inside these `prcomp` objects?

First, the rotation matrix gives the weights of each variable for each principal component:

```{r}

pca$rotation



```

Second, we can extract the projection of each datapoint onto these components:

```{r}
pca$x
#use x to predict on your training data; use predict function on your test data
```

Now we can do some plotting:

```{r}
#bind principal components to OG dataset
d_pca <-  bind_cols(d_analysis, pca$x)

#plot the first 2 PCs against each other
ggplot(d_pca, aes(x=PC1, y=PC2)) +
  geom_point()+
  labs(title='Plotting first two PCs') +
  theme_bw()

```

Now plot outcome vs principal components

```{r}

ggplot(d_pca, aes(x=PC1, y=sch_satisfaction)) +
  stat_summary(geom = "point", fun = "mean")+
  labs(title='Plotting happiness vs first PC') +
  theme_bw()

```

```{r}

ggplot(d_pca, aes(x=PC2, y=sch_satisfaction)) +
  stat_summary(geom = "point", fun = "mean") +
  labs(title='Plotting happiness vs second PC') +
  theme_bw()

```

```{r}

library(FactoMineR)

# Create a dataset with mixed variables (categorical and numeric)
d_famd <- d_proto |> mutate(neg_isolate = if_else(neg_isolate == '0', 'has neg ties', 'no neg ties'),
                            pos_isolate = if_else(pos_isolate == '0', 'has pos ties', 'no pos ties'))

# Perform FAMD on the mixed data
result <- FAMD(d_famd, ncp = 5, graph = FALSE)

# Access the results
summary(result)  # Summary of the FAMD analysis
result$var$coord


variable_contributions <- dimdesc(result, axes = c(1, 2, 3, 4, 5), proba = 0.05)
print(variable_contributions)
result$eig


eig <- get_eigenvalue(result)  # Eigenvalues and variance explained
eig$eigenvalue
eig$variance
```

## Modeling

```{r}

## Using mixed model with penalized quasilikelihood (PQL) since our data are not normally distributed

PQL <- glmmPQL(sch_satisfaction ~  PC1 + PC2 + PC3 + sexgirl + lang_test + cog_test + age + immigrant + ISEI_parent_c + class_size, random = ~1 | schoolidCUF/classidCUF, family = Gamma(link = "log"),
    data = d_pca, verbose = FALSE)
summary(PQL)


```

# Visualizations

```{r}
ggplot(data = d_analysis, 
       aes(x = fct_reorder(as.character(classidCUF), 
                           sch_satisfaction),
           y = sch_satisfaction)) + 
  geom_point(size = 0.5,
             alpha = 0.5) +
  stat_summary(fun = "mean", 
               geom = "point", 
               shape = 21, 
               size = 2, 
               fill = "darkblue",
               color = "#4D4F53")+
  labs(y = 'School Satisfaction', x = 'Class ID')+
  theme_classic() +
  theme(axis.text.x = element_blank()) 

```

```{r}
m_ols_b <- list()
m_ols_fv <- list()

unique_classidCUF <- unique(d_analysis$classidCUF)
for (i in unique_classidCUF){
  myclass <- filter(d_analysis, classidCUF == i)
  mymodel <- lm(sch_satisfaction ~ neg_indegree, data = myclass)
  m_ols_b[[i]] <-  mymodel$coefficients[[2]]
  m_ols_fv[[i]] <- mymodel$fitted.values
}  


d_analysis |>
  mutate(sch_satisfaction_hat_ols = unlist(m_ols_fv)) |>
  ggplot(aes(x = neg_indegree, 
             y = sch_satisfaction_hat_ols, 
             group = as.factor(classidCUF))) + 
  geom_line() + 
  theme(legend.position = "none") + 
  xlab("Neg-Indegree") +
  ylab("Predicted School Satisfaction") +
  ggtitle("Sample of Random Slopes \n(Negatative Indegree Across Classes")+
  theme_classic()
```

```{r}
d_variables <- d_analysis |> 
  dplyr::dplyr::select(sexgirl, age, immigrant, victimization, schooltype, class_size, lang_test, cog_test, neg_indegree, neg_outdegree, pos_indegree, pos_outdegree, pos_isolate, neg_isolate, n_mutual_bfs, non_reciprocal, sch_satisfaction) |> 
  na.omit()
summary(d_variables)

stargazer(as.data.frame(d_variables), type = "text", out = "summarystats.txt",
          title = "Table 1. Descriptive Statistics", digits = 2)


install.packages("table1")
library(table1)
table1(d_variables)

library(psych)
table1 <- describe(d_variables)

```
